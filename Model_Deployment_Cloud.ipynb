{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/A-Chillarige/MedicalImageClassification/blob/main/Model_Deployment_Cloud.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyngrok"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xPHTRka8mIar",
        "outputId": "d26da4a5-35c4-4af3-e807-e9382dab1d69"
      },
      "id": "xPHTRka8mIar",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyngrok\n",
            "  Downloading pyngrok-7.2.1-py3-none-any.whl.metadata (8.3 kB)\n",
            "Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.10/dist-packages (from pyngrok) (6.0.2)\n",
            "Downloading pyngrok-7.2.1-py3-none-any.whl (22 kB)\n",
            "Installing collected packages: pyngrok\n",
            "Successfully installed pyngrok-7.2.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "2c89fc06-eefa-4579-9650-7f95d1cd8a97",
      "metadata": {
        "id": "2c89fc06-eefa-4579-9650-7f95d1cd8a97"
      },
      "outputs": [],
      "source": [
        "from flask import Flask, request, jsonify\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.transforms as transforms\n",
        "from PIL import Image\n",
        "import requests\n",
        "from urllib.request import urlopen\n",
        "import timm\n",
        "import io\n",
        "from io import BytesIO\n",
        "from pyngrok import ngrok\n",
        "import threading"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0ce65b00-b462-49a4-b676-f7394c9d1896",
      "metadata": {
        "id": "0ce65b00-b462-49a4-b676-f7394c9d1896"
      },
      "source": [
        "## Flask API Guide\n",
        "\n",
        "### How the Following Code Works\n",
        "##### First, the Model is loaded and fitted using the trained weights\n",
        "##### Then, the following functions are used to put the model into use:\n",
        "\n",
        "- ##### The model takes in an image path\n",
        "  \n",
        "- ##### After ensuring an acceptable file is sent, it is then read and sent to the image_transformation function\n",
        "    - This function takes the contents of the image file and wraps it in a steam, allowing you to treat it like a file object\n",
        "    - Then, it changes the image to RGBA, resizes it, transforms it to a tensor, and normalizes it before adding a batch dimension and placing it in the variable tensor\n",
        "    - These transformations ensure the image sent is in the same format as the images the model was trained on\n",
        "          \n",
        "- ##### The variable tensor is then sent to the get_prediction function\n",
        "    - This sends the tensor to the model\n",
        "    - Selects the output class with the highest probability\n",
        "    - Matches the output class with its label name and finally returns the label name.\n",
        "      \n",
        "- ##### Then, the label name is put into a dictionary, which is sent to the user\n",
        "###\n",
        "\n",
        "### How to use this model:\n",
        "![Screenshot 2024-11-12 at 6.18.50â€¯PM.png](attachment:6afbc858-0f0f-4f94-ad4b-cabedb7bacd4.png)\n",
        "#### First run the code in this notebook\n",
        "#### Then in a new notebook, do the following:\n",
        "\n",
        "Import requests\n",
        "* Create a response variable and send a post request to the host: \"http://127.0.0.1:5000/predict\"\n",
        "    * In this case, the response variable is \"resp\"<p/>\n",
        "\n",
        "* Inside the post request, also send a file dictionary in the following format: files = {'file': open(image_path, 'rb')}\n",
        "* Print the response text"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### This code is copied from Pygroks google colab example\n",
        "\n",
        "\n",
        "import getpass\n",
        "\n",
        "from pyngrok import ngrok, conf\n",
        "\n",
        "print(\"Enter your authtoken, which can be copied from https://dashboard.ngrok.com/auth\")\n",
        "conf.get_default().auth_token = getpass.getpass()\n",
        "\n",
        "# Open a TCP ngrok tunnel to the SSH server\n",
        "connection_string = ngrok.connect(\"22\", \"tcp\").public_url\n",
        "\n",
        "ssh_url, port = connection_string.strip(\"tcp://\").split(\":\")\n",
        "print(f\" * ngrok tunnel available, access with `ssh root@{ssh_url} -p{port}`\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zMA3LX_WSSRk",
        "outputId": "32d805cc-9f7d-4413-cee6-2e3fa358b0e0"
      },
      "id": "zMA3LX_WSSRk",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter your authtoken, which can be copied from https://dashboard.ngrok.com/auth\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "8007f05d-d338-428f-888d-76278b5446c7",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8007f05d-d338-428f-888d-76278b5446c7",
        "outputId": "f76e79aa-81f0-48f8-e024-7546f98567c4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " * ngrok tunnel \"https://4701-34-125-221-90.ngrok-free.app\" -> \"http://127.0.0.1:5000\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-15-88623c59cb37>:14: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load(PATH))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " * Serving Flask app '__main__'\n",
            " * Debug mode: off\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:werkzeug:\u001b[31m\u001b[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\u001b[0m\n",
            " * Running on http://127.0.0.1:5000\n",
            "INFO:werkzeug:\u001b[33mPress CTRL+C to quit\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "app = Flask(__name__)\n",
        "\n",
        "### This code is copied from Pygroks google colab example https://pyngrok.readthedocs.io/en/latest/integrations.html#google-colaboratory\n",
        "### Copied code ends at ###\n",
        "port = '5000'\n",
        "public_url = ngrok.connect(port).public_url\n",
        "print(f\" * ngrok tunnel \\\"{public_url}\\\" -> \\\"http://127.0.0.1:{port}\\\"\")\n",
        "\n",
        "app.config[\"BASE_URL\"] = public_url\n",
        "####\n",
        "\n",
        "model = timm.create_model('densenet121.ra_in1k', pretrained=False)\n",
        "model.features[0] = nn.Conv2d(in_channels=4, out_channels=64, kernel_size=(7,7), stride=(2,2), padding=(3,3), bias=False)\n",
        "model.classifier = nn.Linear(1024, 6)\n",
        "PATH = \"MedicalClassification.pth\"\n",
        "model.load_state_dict(torch.load(PATH))\n",
        "model.eval()\n",
        "\n",
        "def image_transformation(image_bytes):\n",
        "        image_data = BytesIO(image_bytes)\n",
        "        image_data.seek(0)\n",
        "        img = Image.open(image_data)\n",
        "\n",
        "        transform = transforms.Compose(\n",
        "                [\n",
        "                    transforms.Lambda(lambda img: img.convert(\"RGBA\")),\n",
        "                    transforms.Resize((224,224)),\n",
        "                    transforms.ToTensor(),\n",
        "                    transforms.Normalize(mean = [-5.0247e-05, -2.3672e-05,  2.9741e-05,  3.6599e+00], std=[0.9986, 0.9986, 0.9987, 0.0011])\n",
        "                ]\n",
        "            )\n",
        "        transformed_img = transform(img).unsqueeze(0)\n",
        "        return transformed_img\n",
        "\n",
        "\n",
        "def get_prediction(image_tensor):\n",
        "    out = model(image_tensor)\n",
        "    predictions = torch.argmax(out, axis =1)\n",
        "    Label_dict = {0:\"Not Fractured\", 1: \"Fractured Bone\", 2: \"No Tumor\", 3:\"Glioma\", 4:\"Meningioma\", 5:\"Pituitary\"}\n",
        "    Final_prediction = Label_dict[predictions.item()]\n",
        "    return Final_prediction\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "ALLOWED_EXTENSIONS = {'png', 'jpg', 'jpeg'}\n",
        "def allowed_file(filename):\n",
        "    return '.' in filename and filename.rsplit('.',1)[1].lower() in ALLOWED_EXTENSIONS\n",
        "\n",
        "\n",
        "@app.route('/predict', methods = ['POST'])\n",
        "def predict():\n",
        "    if request.method == 'POST':\n",
        "        file = request.files.get('file')\n",
        "        if file is None or file.filename == \"\":\n",
        "            return jsonify({'Error': 'No File'})\n",
        "        if not allowed_file(file.filename):\n",
        "            return jsonify({'Error': 'File Format Not Supported'})\n",
        "        try:\n",
        "            img_bytes = file.read()\n",
        "            tensor = image_transformation(img_bytes)\n",
        "            prediction = get_prediction(tensor)\n",
        "            data = {'prediction':str(prediction)}\n",
        "            return jsonify(data)\n",
        "\n",
        "\n",
        "        except:\n",
        "            return jsonify({'Error': 'Error During Prediction'})\n",
        "\n",
        "    return jsonify({'result': 1})\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "### This section of code is also copied from Pygroks google colab example https://pyngrok.readthedocs.io/en/latest/integrations.html#google-colaboratory\n",
        "### Copied code ends at ###\n",
        "\n",
        "  threading.Thread(target=app.run, kwargs={\"use_reloader\": False}).start()\n",
        "###"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}